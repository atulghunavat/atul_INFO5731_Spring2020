{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "In_class_exercise_05_Atul (1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atulghunavat/atul_INFO5731_Spring2020/blob/master/In_class_exercise/In_class_exercise_05_Atul.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7TahL04sVvR",
        "colab_type": "text"
      },
      "source": [
        "# **The fifth in-class-exercise**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejyZITr8sjnh",
        "colab_type": "text"
      },
      "source": [
        "## **1. Rule-based information extraction**\n",
        "\n",
        "Use any keywords related to data science, natural language processing, machine learning to search from google scholar, get the **titles** of 100 articles (either by web scraping or manually) about this topic, define a set of patterns to extract the research questions/problems, methods/algorithms/models, datasets, applications, or any other important information about this topic. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvR_O9D8sOUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install scholarly"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keS4ZiRPkO1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scholarly\n",
        "import pandas as pd\n",
        "\n",
        "title_df = pd.DataFrame(columns=['title'])\n",
        "search_q = scholarly.search_pubs_query('Deep Learning')\n",
        "count = 1\n",
        "title_list = []\n",
        "for a in search_q:\n",
        "    title = a.bib['title']\n",
        "    title_list.append(title)\n",
        "    print(str(count) + (a.bib['title']))\n",
        "    if(count >= 100):\n",
        "      break\n",
        "    count = count + 1\n",
        "title_df['title'] = title_list\n",
        "title_df.to_csv('/content/title.csv') \n",
        "\n",
        "#Created a CSV by scraping Google Scholar and loaded it into a dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fd-aHof9ww17",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "99c29e7b-b0da-45d1-9c80-d971647b6156"
      },
      "source": [
        "import scholarly\n",
        "import pandas as pd\n",
        "\n",
        "title_df_from_csv = pd.read_csv('/content/title.csv')\n",
        "title_list = title_df_from_csv['title']\n",
        "\n",
        "for title in title_list:\n",
        "  print(title)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deep learning\n",
            "Deep learning\n",
            "Multimodal deep learning\n",
            "Deep learning in neural networks: An overview\n",
            "Deep learning: methods and applications\n",
            "Deep learning\n",
            "Deep learning for health informatics\n",
            "Why does unsupervised pre-training help deep learning?\n",
            "Neural networks and deep learning\n",
            "Playing atari with deep reinforcement learning\n",
            "Privacy-preserving deep learning\n",
            "Deep learning via hessian-free optimization.\n",
            "cudnn: Efficient primitives for deep learning\n",
            "Deep learning-based classification of hyperspectral data\n",
            "A survey on deep learning in medical image analysis\n",
            "Deep learning for visual understanding: A review\n",
            "Collaborative deep learning for recommender systems\n",
            "Deep learning\n",
            "Deep learning face attributes in the wild\n",
            "Deep learning with limited numerical precision\n",
            "Deep learning in medical image analysis\n",
            "Wide & deep learning for recommender systems\n",
            "Understanding deep learning requires rethinking generalization\n",
            "On optimization methods for deep learning\n",
            "Deep learning with differential privacy\n",
            "Geometric deep learning: going beyond euclidean data\n",
            "Xception: Deep learning with depthwise separable convolutions\n",
            "Deep learning in bioinformatics\n",
            "Deep learning face representation by joint identification-verification\n",
            "Deep learning of representations for unsupervised and transfer learning\n",
            "Deep learning of representations: Looking forward\n",
            "Chainer: a next-generation open source framework for deep learning\n",
            "The limitations of deep learning in adversarial settings\n",
            "Predicting parameters in deep learning\n",
            "Deep learning via semi-supervised embedding\n",
            "Dropout as a bayesian approximation: Representing model uncertainty in deep learning\n",
            "Deep learning without poor local minima\n",
            "Deep learning face representation from predicting 10,000 classes\n",
            "Towards deep learning models resistant to adversarial attacks\n",
            "On the importance of initialization and momentum in deep learning\n",
            "Deep learning applications and challenges in big data analytics\n",
            "Deep learning using linear support vector machines\n",
            "Unsupervised feature learning for audio classification using convolutional deep belief networks\n",
            "Deep learning and education for sustainability\n",
            "Deep learning with COTS HPC systems\n",
            "Kernel methods for deep learning\n",
            "Traffic flow prediction with big data: a deep learning approach\n",
            "Joint deep learning for pedestrian detection\n",
            "Uncertainty in deep learning\n",
            "Deep learning: A critical appraisal\n",
            "Deep learning in agriculture: A survey\n",
            "Deep metric learning using triplet network\n",
            "Pointnet: Deep learning on point sets for 3d classification and segmentation\n",
            "Recent trends in deep learning based natural language processing\n",
            "PCANet: A simple deep learning baseline for image classification?\n",
            "Recent advances in deep learning for speech research at Microsoft\n",
            "Deep learning with coherent nanophotonic circuits\n",
            "Deep learning for content-based image retrieval: A comprehensive study\n",
            "Deep learning: A practitioner's approach\n",
            "Big data deep learning: challenges and perspectives\n",
            "Domain adaptation for large-scale sentiment classification: A deep learning approach\n",
            "Saliency detection by multi-context deep learning\n",
            "An improved deep learning architecture for person re-identification\n",
            "Theano: Deep learning on gpus with python\n",
            "Sequential deep learning for human action recognition\n",
            "Deep learning for computational biology\n",
            "Deep learning and the information bottleneck principle\n",
            "A review of unsupervised feature learning and deep learning for time-series modeling\n",
            "Predicting the sequence specificities of DNA-and RNA-binding proteins by deep learning\n",
            "CNTK: Microsoft's open-source deep-learning toolkit\n",
            "Convolutional-recursive deep learning for 3d object classification\n",
            "A guide to convolution arithmetic for deep learning\n",
            "Deep learning in remote sensing: A comprehensive review and list of resources\n",
            "Deep learning for identifying metastatic breast cancer\n",
            "Learning deep representations for graph clustering\n",
            "A tutorial survey of architectures, algorithms, and applications for deep learning\n",
            "Deep learning microscopy\n",
            "Searching for exotic particles in high-energy physics with deep learning\n",
            "Deep learning for monaural speech separation\n",
            "What uncertainties do we need in bayesian deep learning for computer vision?\n",
            "An introduction to deep learning for the physical layer\n",
            "An empirical evaluation of deep learning on highway driving\n",
            "Deep learning for detecting robotic grasps\n",
            "Deep learning with elastic averaging SGD\n",
            "DeepTox: toxicity prediction using deep learning\n",
            "Prediction as a candidate for learning deep hierarchical models of data\n",
            "Using deep learning for image-based plant disease detection\n",
            "On large-batch training for deep learning: Generalization gap and sharp minima\n",
            "Project adam: Building an efficient and scalable deep learning training system\n",
            "Relational inductive biases, deep learning, and graph networks\n",
            "Comparative theology: Deep learning across religious borders\n",
            "Deep learning based recommender system: A survey and new perspectives\n",
            "Unsupervised feature learning and deep learning: A review and new perspectives\n",
            "Geometric deep learning on graphs and manifolds using mixture model cnns\n",
            "Deep learning and its applications to machine health monitoring\n",
            "Deep bayesian active learning with image data\n",
            "Guest editorial deep learning in medical imaging: Overview and future promise of an exciting new technique\n",
            "Efficient deep learning for stereo matching\n",
            "Hybrid deep learning for face verification\n",
            "Predicting effects of noncoding variants with deep learning–based sequence model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZI3H1PWzeBZ",
        "colab_type": "code",
        "outputId": "89cbffa6-984a-49fc-f069-268be4e87881",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        }
      },
      "source": [
        "import re\n",
        "import string \n",
        "import nltk \n",
        "import spacy \n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import math \n",
        "from tqdm import tqdm \n",
        "\n",
        "from spacy.matcher import Matcher \n",
        "from spacy.tokens import Span \n",
        "from spacy import displacy \n",
        "\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "# load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "pattern1 = [{'POS':'NOUN'}, \n",
        "           {'LOWER': 'and'}, \n",
        "           {'LOWER':'the','OP':\"?\"},\n",
        "           {'POS': 'NOUN'}] \n",
        "\n",
        "# A token whose lowercase form matches such, e.g. “and”.\n",
        "print(\"-------Pattern 1 'X and Y'--------\")\n",
        "for title in title_list:\n",
        "  doc = nlp(title)\n",
        "# Matcher class object \n",
        "  matcher = Matcher(nlp.vocab) \n",
        "  matcher.add(\"matching_1\", None, pattern1) \n",
        "  matches = matcher(doc) \n",
        "  if(len(matches) > 0): \n",
        "    span = doc[matches[0][1]:matches[0][2]] \n",
        "    print(span.text)\n",
        "\n",
        "pattern2 = [{'POS':'NOUN'}, \n",
        "           {'LOWER': 'using'},\n",
        "           {'POS':'ADJ','OP':\"*\"},\n",
        "           {'POS': 'NOUN'}] \n",
        "\n",
        "print(\"\\n-------Pattern 2 'X using Y'--------\")\n",
        "for title in title_list:\n",
        "  doc = nlp(title)\n",
        "\n",
        "# Matcher class object \n",
        "  matcher = Matcher(nlp.vocab) \n",
        "  matcher.add(\"matching_1\", None, pattern2) \n",
        "  matches = matcher(doc) \n",
        "  if(len(matches) > 0): \n",
        "    span = doc[matches[0][1]:matches[0][2]] \n",
        "    print(span.text)\n",
        "\n",
        "\n",
        "pattern3 = [{'POS':'NOUN'}, \n",
        "           {'LOWER': 'with'},\n",
        "           {'POS':'ADJ','OP':\"*\"},\n",
        "           {'POS': 'NOUN'}] \n",
        "\n",
        "print(\"\\n-------Pattern 3 'X with Y'--------\")\n",
        "for title in title_list:\n",
        "  doc = nlp(title)\n",
        "\n",
        "# Matcher class object \n",
        "  matcher = Matcher(nlp.vocab) \n",
        "  matcher.add(\"matching_1\", None, pattern3) \n",
        "  matches = matcher(doc) \n",
        "  if(len(matches) > 0): \n",
        "    span = doc[matches[0][1]:matches[0][2]] \n",
        "    print(span.text)\n",
        "\n",
        "pattern4 = [{'POS':'ADJ'},\n",
        "            {'POS':'VERB'}, \n",
        "            {'LOWER': 'via'},\n",
        "            {'POS':'ADJ'},\n",
        "            {'POS':'PUNCT', 'OP':'*'},\n",
        "            {'POS':'ADJ'},\n",
        "            {'POS': 'NOUN'}] \n",
        "\n",
        "print(\"\\n-------Pattern 4 'X via Y'--------\")\n",
        "for title in title_list:\n",
        "  doc = nlp(title)\n",
        "# Matcher class object \n",
        "  matcher = Matcher(nlp.vocab) \n",
        "  matcher.add(\"matching_1\", None, pattern4) \n",
        "  matches = matcher(doc) \n",
        "  if(len(matches) > 0): \n",
        "    span = doc[matches[0][1]:matches[0][2]] \n",
        "    print(span.text)\n",
        "\n",
        "pattern5 = [{'POS':'NOUN', 'OP':\"+\"}, \n",
        "            {'POS':'VERB', 'OP':\"*\"},\n",
        "           {'LOWER': 'for'},\n",
        "           {'POS':'ADJ','OP':\"*\"},\n",
        "           {'POS': 'NOUN'},\n",
        "          {'POS': 'NOUN','OP':\"?\"}] \n",
        "print(\"\\n-------Pattern 5 'X for Y'--------\")\n",
        "for title in title_list:\n",
        "  doc = nlp(title)\n",
        "# Matcher class object \n",
        "  matcher = Matcher(nlp.vocab) \n",
        "  matcher.add(\"matching_1\", None, pattern5) \n",
        "  matches = matcher(doc) \n",
        "  if(len(matches) > 0): \n",
        "    span = doc[matches[0][1]:matches[0][2]] \n",
        "    print(span.text)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------Pattern 1 'X and Y'--------\n",
            "methods and applications\n",
            "initialization and momentum\n",
            "applications and challenges\n",
            "learning and education\n",
            "classification and segmentation\n",
            "challenges and perspectives\n",
            "learning and the information\n",
            "review and list\n",
            "graphs and manifolds\n",
            "\n",
            "-------Pattern 2 'X using Y'--------\n",
            "learning using linear support\n",
            "classification using convolutional deep belief\n",
            "prediction using deep learning\n",
            "manifolds using mixture model\n",
            "\n",
            "-------Pattern 3 'X with Y'--------\n",
            "learning with differential privacy\n",
            "prediction with big data\n",
            "gpus with python\n",
            "physics with deep learning\n",
            "learning with elastic averaging\n",
            "variants with deep learning\n",
            "\n",
            "-------Pattern 4 'X via Y'--------\n",
            "Deep learning via hessian-free optimization\n",
            "Deep learning via semi-supervised embedding\n",
            "\n",
            "-------Pattern 5 'X for Y'--------\n",
            "learning for health informatics\n",
            "primitives for deep learning\n",
            "learning for visual understanding\n",
            "learning for recommender systems\n",
            "learning for recommender systems\n",
            "methods for deep learning\n",
            "framework for deep learning\n",
            "feature learning for audio classification\n",
            "education for sustainability\n",
            "methods for deep learning\n",
            "learning for pedestrian detection\n",
            "baseline for image classification\n",
            "learning for speech research\n",
            "learning for content\n",
            "architecture for person re\n",
            "learning for human action recognition\n",
            "learning for computational biology\n",
            "learning for time\n",
            "representations for graph clustering\n",
            "applications for deep learning\n",
            "learning for monaural speech separation\n",
            "learning for computer vision\n",
            "learning for image\n",
            "training for deep learning\n",
            "learning for stereo matching\n",
            "learning for face verification\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq_7VGmrsum4",
        "colab_type": "text"
      },
      "source": [
        "## **2. Domain-specific information extraction**\n",
        "\n",
        "For the legal case used in the data cleaning exercise: [01-05-1 Adams v Tanner.txt](https://raw.githubusercontent.com/unt-iialab/INFO5731_Spring2020/master/In_class_exercise/01-05-1%20%20Adams%20v%20Tanner.txt), use [legalNLP](https://lexpredict-lexnlp.readthedocs.io/en/latest/modules/extract/extract.html#nlp-based-extraction-methods) to extract the following inforation from the text (if the information is not exist, just print None):\n",
        "\n",
        "(1) acts, e.g., “section 1 of the Advancing Hope Act, 1986”\n",
        "\n",
        "(2) amounts, e.g., “ten pounds” or “5.8 megawatts”\n",
        "\n",
        "(3) citations, e.g., “10 U.S. 100” or “1998 S. Ct. 1”\n",
        "\n",
        "(4) companies, e.g., “Lexpredict LLC”\n",
        "\n",
        "(5) conditions, e.g., “subject to …” or “unless and until …”\n",
        "\n",
        "(6) constraints, e.g., “no more than”\n",
        "\n",
        "(7) copyright, e.g., “(C) Copyright 2000 Acme”\n",
        "\n",
        "(8) courts, e.g., “Supreme Court of New York”\n",
        "\n",
        "(9) CUSIP, e.g., “392690QT3”\n",
        "\n",
        "(10) dates, e.g., “June 1, 2017” or “2018-01-01”\n",
        "\n",
        "(11) definitions, e.g., “Term shall mean …”\n",
        "\n",
        "(12) distances, e.g., “fifteen miles”\n",
        "\n",
        "(13) durations, e.g., “ten years” or “thirty days”\n",
        "\n",
        "(14) geographic and geopolitical entities, e.g., “New York” or “Norway”\n",
        "\n",
        "(15) money and currency usages, e.g., “$5” or “10 Euro”\n",
        "\n",
        "(16) percents and rates, e.g., “10%” or “50 bps”\n",
        "\n",
        "(17) PII, e.g., “212-212-2121” or “999-999-9999”\n",
        "\n",
        "(18) ratios, e.g.,” 3:1” or “four to three”\n",
        "\n",
        "(19) regulations, e.g., “32 CFR 170”\n",
        "\n",
        "(20) trademarks, e.g., “MyApp (TM)”\n",
        "\n",
        "(21) URLs, e.g., “http://acme.com/”\n",
        "\n",
        "(22) addresses, e.g., “1999 Mount Read Blvd, Rochester, NY, USA, 14615”\n",
        "\n",
        "(23) persons, e.g., “John Doe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc7NtJrLx5tS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import requests\n",
        "page = 'https://raw.githubusercontent.com/unt-iialab/INFO5731_Spring2020/master/In_class_exercise/01-05-1%20%20Adams%20v%20Tanner.txt'\n",
        "data = requests.get(page).text\n",
        "print (data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BAVE14CQGiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install lexnlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NqGROdcbr84",
        "colab_type": "code",
        "outputId": "9852b4e9-558a-4493-f5ac-bc0d00ec36ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bI57nwecSJSh",
        "colab_type": "code",
        "outputId": "bb589364-08a9-440c-c2d2-4d64d7110048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Acts\n",
        "import lexnlp.extract.en.acts\n",
        "isnone = True\n",
        "for x in data.split('\\n'):\n",
        "  a = list(lexnlp.extract.en.acts.get_act_list(x))\n",
        "  if len(a) != 0:\n",
        "    isnone = False\n",
        "    print(a)\n",
        "\n",
        "if isnone:\n",
        "    print('None')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzr-9m6TbYvW",
        "colab_type": "code",
        "outputId": "352507bb-af9e-4458-bf4d-ac9bf0bcb029",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Amounts\n",
        "import lexnlp.extract.en.amounts\n",
        "isnone = True\n",
        "for x in data.split(' '):\n",
        "  if len(x) > 0:\n",
        "    a = list(lexnlp.extract.en.amounts.get_amounts(x))\n",
        "    if len(a) != 0:\n",
        "      isnone = False\n",
        "      print(a)\n",
        "\n",
        "if isnone:\n",
        "    print('None')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5.0]\n",
            "[740.0]\n",
            "[1843.0]\n",
            "[2.0, 1.0]\n",
            "[4.0]\n",
            "[2.0]\n",
            "[1821.0]\n",
            "[5.0]\n",
            "[1.0]\n",
            "[1840.0]\n",
            "[37]\n",
            "[100]\n",
            "[77]\n",
            "[80.0, 100.0]\n",
            "[30]\n",
            "[1839.0]\n",
            "[741.0]\n",
            "[22]\n",
            "[1840.0]\n",
            "[14]\n",
            "[1000]\n",
            "[1]\n",
            "[100]\n",
            "[20]\n",
            "[1]\n",
            "[1840.0]\n",
            "[3]\n",
            "[4]\n",
            "[1]\n",
            "[1.0]\n",
            "[1840.0]\n",
            "[2.0]\n",
            "[1.0]\n",
            "[361.0]\n",
            "[1.0]\n",
            "[307.0]\n",
            "[6.0]\n",
            "[604.0]\n",
            "[1.0]\n",
            "[2.0]\n",
            "[418.0]\n",
            "[422.0]\n",
            "[7.0]\n",
            "[34.0]\n",
            "[41.0]\n",
            "[167.0]\n",
            "[742.0]\n",
            "[3.0]\n",
            "[112.0]\n",
            "[207.0]\n",
            "[3.0]\n",
            "[338.0]\n",
            "[424.0]\n",
            "[5.0]\n",
            "[26.0]\n",
            "[13.0]\n",
            "[235.0]\n",
            "[8.0]\n",
            "[693.0]\n",
            "[4.0]\n",
            "[1821.0]\n",
            "[167.0]\n",
            "[2.0]\n",
            "[2.0]\n",
            "[216.0]\n",
            "[3.0]\n",
            "[66.0]\n",
            "[4.0]\n",
            "[130.0]\n",
            "[29.0]\n",
            "[2.0]\n",
            "[241.0, 2.0]\n",
            "[332.0]\n",
            "[2.0]\n",
            "[422.0]\n",
            "[9.0]\n",
            "[112.0]\n",
            "[743.0]\n",
            "[9.0]\n",
            "[39.0]\n",
            "[14]\n",
            "[1000]\n",
            "[1840.0]\n",
            "[744.0]\n",
            "[5.0]\n",
            "[182.0, 3.0]\n",
            "[368.0]\n",
            "[1.0]\n",
            "[397.0]\n",
            "[6.0]\n",
            "[604.0]\n",
            "[1]\n",
            "[1821.0]\n",
            "[167.0]\n",
            "[745.0]\n",
            "[4.0]\n",
            "[746.0]\n",
            "[4.0]\n",
            "[210.0]\n",
            "[46.0]\n",
            "[747.0]\n",
            "[5.0]\n",
            "[5.0]\n",
            "[740.0]\n",
            "[1843.0]\n",
            "[284.0]\n",
            "[2019.0]\n",
            "[9.0]\n",
            "[1.0]\n",
            "[55.0]\n",
            "[266.0]\n",
            "[271.0]\n",
            "[1876.0]\n",
            "[2.0]\n",
            "[47.0]\n",
            "[362.0]\n",
            "[376.0]\n",
            "[1872.0]\n",
            "[3.0]\n",
            "[45.0]\n",
            "[329.0]\n",
            "[334.0]\n",
            "[1871.0]\n",
            "[4.0]\n",
            "[31.0]\n",
            "[526.0]\n",
            "[527.0]\n",
            "[1858.0]\n",
            "[5.0]\n",
            "[21.0]\n",
            "[333.0]\n",
            "[335.0]\n",
            "[1852.0]\n",
            "[6.0]\n",
            "[8.0]\n",
            "[145.0]\n",
            "[147.0]\n",
            "[1857.0]\n",
            "[7.0]\n",
            "[65.0]\n",
            "[256.0]\n",
            "[258.0]\n",
            "[3]\n",
            "[1880.0]\n",
            "[8.0]\n",
            "[4.0]\n",
            "[913.0]\n",
            "[914.0]\n",
            "[1887.0, 9.0]\n",
            "[103.0]\n",
            "[464.0]\n",
            "[1936.0]\n",
            "[3.0]\n",
            "[1.0]\n",
            "[9.0]\n",
            "[39.0]\n",
            "[1828.0]\n",
            "[2.0]\n",
            "[2.0]\n",
            "[5.0]\n",
            "[182.0]\n",
            "[1837.0]\n",
            "[2.0, 3.0]\n",
            "[9.0]\n",
            "[108.0]\n",
            "[1812.0]\n",
            "[6]\n",
            "[1]\n",
            "[2.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lvhjCath6Y-",
        "colab_type": "code",
        "outputId": "8ae47f85-025f-48ad-a995-6279e8dce474",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "#Citations\n",
        "import lexnlp.extract.en.citations\n",
        "isnone = True\n",
        "for x in data.split('\\n'):\n",
        "  if len(x) > 0:\n",
        "    a = list(lexnlp.extract.en.citations.get_citations(x))\n",
        "    if len(a) != 0:\n",
        "      isnone = False\n",
        "      print(a)\n",
        "\n",
        "if isnone:\n",
        "    print('None')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(5, 'Ala.', 'Alabama Reports', 740, None, None, None)]\n",
            "[(5, 'Ala.', 'Alabama Reports', 740, '1843', None, None)]\n",
            "[(55, 'Ala.', 'Alabama Reports', 266, '271', None, None)]\n",
            "[(47, 'Ala.', 'Alabama Reports', 362, '376', None, None)]\n",
            "[(45, 'Ala.', 'Alabama Reports', 329, '334', None, None)]\n",
            "[(31, 'Ala.', 'Alabama Reports', 526, '527', None, None)]\n",
            "[(21, 'Ala.', 'Alabama Reports', 333, '335', None, None)]\n",
            "[(8, 'Cal.', 'California Reports', 145, '147', None, None)]\n",
            "[(65, 'Ala.', 'Alabama Reports', 256, '258', None, None)]\n",
            "[(4, 'S.W.', 'South Western Reporter', 913, '914', None, None)]\n",
            "[(103, 'A.L.R.', 'American Law Reports', 464, None, None, None)]\n",
            "[(9, 'Cow.', \"Cowen's Reports\", 39, None, None, None)]\n",
            "[(5, 'Port.', 'Alabama Reports, Porter', 182, None, None, None)]\n",
            "[(9, 'Johns.', \"Johnson's Reports\", 108, None, None, None)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfgODH9Eiy3C",
        "colab_type": "code",
        "outputId": "141aab54-52cb-4743-8d50-41f9f2fb310b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Companies\n",
        "import lexnlp.extract.en.entities.nltk_re\n",
        "isnone = True\n",
        "for x in data.split('\\n'):\n",
        "  if len(x) > 0:\n",
        "    a = list(lexnlp.extract.en.entities.nltk_re.get_companies(x))\n",
        "    if len(a) != 0:\n",
        "      isnone = False\n",
        "      print(a)\n",
        "\n",
        "if isnone:\n",
        "    print('None')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Lehman, Durr Co, (2, 19)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzxr1ik6lxPy",
        "colab_type": "code",
        "outputId": "bd436b7f-92c2-4e62-96b4-5e347de50cd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        }
      },
      "source": [
        "#Conditions\n",
        "import lexnlp.extract.en.conditions\n",
        "isnone = True\n",
        "for x in data.split('.'):\n",
        "  if len(x) > 0:\n",
        "    a = list(lexnlp.extract.en.conditions.get_conditions(x))\n",
        "    if len(a) != 0:\n",
        "      isnone = False\n",
        "      print(a)\n",
        "\n",
        "if isnone:\n",
        "    print('None')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('until', '1821, prohibiting a levy on a crop', '')]\n",
            "[('until', 'on a growing crop, nor does such lien attach', '')]\n",
            "[('if', 'Allen Harrison promised and obliged himself to give up his crop to the use of the claimants at any time to save them from suffering as his indorsers;', '')]\n",
            "[('when', 'The claimants came from Tennessee, (where they resided) about the first of September, 1840, bringing with them three or four white laborers, and took possession of the crop and slaves, and with the latter, and white laborers, gathered the cotton, prepared it for market, and', '')]\n",
            "[('if', 'The court charged the jury, that the plaintiff had no lien by virtue of his judgment, and execution on the growing crop; that Harrison had a right to convey it, without being in any manner restrained by them; that the writing adduced, was a sale of the crop, but', ''), ('when', 'it was not, and the lien of the fieri facias would have attached upon it,', ''), ('if', 'gathered, yet', ''), ('not subject to', 'the claimants obtained possession on the first of September, and controlled the gathering of the crop, then no lien attached, and it was', '')]\n",
            "[('until', 'Rep, 693;] and', '')]\n",
            "[('until', '167,] which declares it to be lawful to levy an execution on a planted crop,', '')]\n",
            "[('if', ']\\r\\nIt is admitted that the contract between the defendant in execution, and the claimants, was in good faith,', '')]\n",
            "[('when', 'The defendant in execution might at any time have divested the interest which the contract vested in the claimants, by discharging their liability as his indorsers, or a judgment creditor might have satisfied the lien, and', '')]\n",
            "[('unless', 'We will then consider the writing under which the claimants assert a right, as a mortgage with a power to take possession any time during the year,', '')]\n",
            "[('if', 'Conceding the truth of the facts stated in the bill of exceptions, and we think it will not follow, that the possession of the claimants is a nullity, and that the case must be considered as', '')]\n",
            "[('if', 'The contract contains an express undertaking to give up the crop at any time the claimants might require it for their indemnity, and', ''), ('if', 'they took possession of it in the absence of the grantor, (though without his consent,)', ''), ('if', 'he subsequently acquiesced in it, the inference would be,', '')]\n",
            "[('subject to', 'Dane, in remarking upon this point, says, “The American editor of Bacon’s Abridgment, says, ‘Wheat growing in the ground is a chattel, and', '')]\n",
            "[('until', 'The first section of the act of 1821, “To prevent sheriffs and other officers from levying executions in certain cases, enacts, that “It shall not be lawful for any sheriff or other officer, to levy a writ of fieri facias or other execution on the planted crop of a debtor, or person against whom an execution may issue,', '')]\n",
            "[('until', '] Now here is an express inhibition to levy an execution on a crop while it remains on, or in the ground, and', '')]\n",
            "[('until', 'If so, the act cited, will only have the effect of keeping the right to levy it in abeyance', '')]\n",
            "[('if', 'The lien and the right to levy are intimately connected, and', '')]\n",
            "[('until', 'That it was competent for the legislature to have made it unlawful to levy an execution on particular property,', '')]\n",
            "[('until', 'If the object was merely to suspend the sale,', '')]\n",
            "[('as soon as', 'The idea that the lien attached upon the planted crop', ''), ('until', 'the execution was delivered to the sheriff, though the right to levy it was postponed', '')]\n",
            "[('if', 'They do not refer to the lien,', ''), ('until', 'they did they would postpone it', ''), ('until', 'the crop was gathered; but it is the levy they relate to and postpone', '')]\n",
            "[('until', '**4 The right to levy an execution on a planted crop, then, being expressly taken away by the statute, the lien which is connected with and consequent upon that right, never attaches', '')]\n",
            "[('if', 'The circuit judge may have mistaken the law in supposing that the contract was a sale, but', '')]\n",
            "[('when', 'There is no assumption of any material fact in the charge; but the possession of the claimant, the time', '')]\n",
            "[('if', ', are all referred to the determination of the jury; who are instructed,', '')]\n",
            "[('until', '**4 The statute which presents the question before the court is, that “it shall not be lawful for any sheriff or other officer to levy a writ of fieei facias or other execution, on the planted crop of a debtor, or person against whom an execution may issue,', '')]\n",
            "[('subject to', 'The policy of the State, as indicated by these statutes, is undeniably that all the property of a debtor, real and personal, to which he has a legal title, shall be', '')]\n",
            "[('until', 'The mischief which the statute designed to remedy was, the sacrifice which would be necessarily made by the sale of an immature crop: the statute enables the debtor to retain it', '')]\n",
            "[('if', '**5', '')]\n",
            "[('until', 'The sheriff is forbidden to levy on a “planted crop”', '')]\n",
            "[('if', 'Now,', '')]\n",
            "[('until', 'This, I feel a thorough conviction, was not the intention of the legislature; but that it was to secure him from loss, by prohibiting a levy and sale of the crop,', ''), ('when', 'it was gathered,', '')]\n",
            "[('subject to', 'Growing crops as', '')]\n",
            "[('subject to', '464\\r\\nGenerally, at common law, growing crops raised by annual planting, while still attached to the soil, are regarded as personal chattels,', '')]\n",
            "[('where', 'And', '')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnv12EpRndPH",
        "colab_type": "code",
        "outputId": "61ea42bf-4848-4ed7-d302-bfd63f839e22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "#Contraints\n",
        "import lexnlp.extract.en.constraints\n",
        "isnone = True\n",
        "for x in data.split('\\n'):\n",
        "  if len(x) > 0:\n",
        "    a = list(lexnlp.extract.en.constraints.get_constraints(x))\n",
        "    if len(a) != 0:\n",
        "      isnone = False\n",
        "      print(a)\n",
        "\n",
        "if isnone:\n",
        "    print('None')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('after', 'on a growing crop, nor does such lien attach until', '')]\n",
            "[('after', '', ' and that alias and pluries fieri facias’, issued regularly up to the time levy was made; that the cotton levied on was growed on the plantation of harrison, and cultivated by the hands in his service.'), ('first of', 'the claimants came from tennessee, (where they resided) about the', '')]\n",
            "[('first of', 'the court charged the jury, that the plaintiff had no lien by virtue of his judgment, and execution on the growing crop; that harrison had a right to convey it, without being in any manner restrained by them; that the writing adduced, was a sale of the crop, but if it was not, and the lien of the fieri facias would have attached upon it, when gathered, yet if the claimants obtained possession on the', '')]\n",
            "[('after', 'it merely inhibits the levy, but the lien attaches, and a levy and sale may be made', '')]\n",
            "[('more than', 'taking this to be clear *744 law, and it will be seen, that the defendant in execution at the time of the levy had nothing', '')]\n",
            "[('before', 'it has been frequently mooted whether, at common law, corn, &c.,', '')]\n",
            "[('before', '**4 the statute which presents the question', '')]\n",
            "[('after', 'now, if the view taken by the majority of the court, is correct, the right secured to the plaintiff in execution, of levying on the crop', '')]\n",
            "[('before', 'tried', '')]\n",
            "[('before', 'tried', '')]\n",
            "[('before', 'tried', '')]\n",
            "[('before', 'tried', '')]\n",
            "[('before', 'tried', '')]\n",
            "[('before', 'tried', '')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpZvSJqAqQR2",
        "colab_type": "code",
        "outputId": "440236fa-7c31-4368-f531-6d1fcd992eab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Copyrights\n",
        "import lexnlp.extract.en.copyright\n",
        "isnone = True\n",
        "for x in data.split('\\n'):\n",
        "  if len(x) > 0:\n",
        "    a = list(lexnlp.extract.en.copyright.get_copyright(x))\n",
        "    if len(a) != 0:\n",
        "      isnone = False\n",
        "      print(a)\n",
        "\n",
        "if isnone:\n",
        "    print('None')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('©', '2019', 'Thomson Reuters. No')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aj4_2OfsTBG",
        "colab_type": "code",
        "outputId": "8f2cdb88-7745-4064-c36f-83f7c86cd49b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#CUSIP\n",
        "import lexnlp.extract.en.cusip\n",
        "# text = \"This is 39298#QT5 code\"\n",
        "isnone = True\n",
        "for x in data.split('\\n'):\n",
        "  a = list(lexnlp.extract.en.cusip.get_cusip(x))\n",
        "  if len(a) != 0:\n",
        "    isnone = False\n",
        "    print(a)\n",
        "\n",
        "if isnone:\n",
        "    print('None')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFE6lUdwAX0J",
        "colab_type": "code",
        "outputId": "12697af3-e785-4e17-8ad6-97ba8107bea0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "#Dates\n",
        "import lexnlp.extract.en.dates\n",
        "\n",
        "isnone = True\n",
        "for x in data.split('\\n'):\n",
        "  a = list(lexnlp.extract.en.dates.get_dates(x))\n",
        "  if len(a) != 0:\n",
        "    isnone = False\n",
        "    print(a)\n",
        "\n",
        "if isnone:\n",
        "    print('None')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[datetime.date(1840, 11, 1), datetime.date(1839, 10, 1), datetime.date(1840, 5, 1), datetime.date(1840, 9, 1)]\n",
            "[datetime.date(1840, 5, 1)]\n",
            "[datetime.date(1840, 5, 1)]\n",
            "[datetime.date(2020, 12, 1)]\n",
            "[datetime.date(1887, 5, 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SwCswYCB5AT",
        "colab_type": "code",
        "outputId": "7ac8a117-caf9-44a0-be5c-7d62b7d3d997",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Definitions\n",
        "import lexnlp.extract.en.definitions\n",
        "\n",
        "isnone = True\n",
        "for x in data.split('\\n'):\n",
        "  a = list(lexnlp.extract.en.definitions.get_definitions(x))\n",
        "  if len(a) != 0:\n",
        "    isnone = False\n",
        "    print(a)\n",
        "\n",
        "if isnone:\n",
        "    print('None')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQsybQ0xJjF9",
        "colab_type": "code",
        "outputId": "985b1b45-8efe-40d9-8d2b-dff2c054482a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Distances\n",
        "import lexnlp.extract.en.distances\n",
        "\n",
        "isnone = True\n",
        "for x in data.split('\\n'):\n",
        "  a = list(lexnlp.extract.en.distances.get_distances(x))\n",
        "  if len(a) != 0:\n",
        "    isnone = False\n",
        "    print(a)\n",
        "\n",
        "if isnone:\n",
        "    print('None')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoEd5bgRJ5Zr",
        "colab_type": "code",
        "outputId": "5ce4932e-4c10-4295-c2d1-2bb074f1f8d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#Durations\n",
        "import lexnlp.extract.en.durations\n",
        "\n",
        "isnone = True\n",
        "for x in data.split('\\n'):\n",
        "  a = list(lexnlp.extract.en.durations.get_durations(x))\n",
        "  if len(a) != 0:\n",
        "    isnone = False\n",
        "    print(a)\n",
        "\n",
        "if isnone:\n",
        "    print('None')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('second', 20.0, 0.00023148148148148146)]\n",
            "[('year', 6.0, 2190.0)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7dIATUkN5te",
        "colab_type": "code",
        "outputId": "02dffaca-d31d-43a7-edf1-cdcbd289e103",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#Currency\n",
        "import lexnlp.extract.en.money\n",
        "\n",
        "isnone = True\n",
        "for x in data.split('\\n'):\n",
        "  a = list(lexnlp.extract.en.money.get_money(x))\n",
        "  if len(a) != 0:\n",
        "    isnone = False\n",
        "    print(a)\n",
        "\n",
        "if isnone:\n",
        "    print('None')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(100.0, 'USD'), (14000, 'USD')]\n",
            "[(14000, 'USD')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5T2WbCIfOg1g",
        "colab_type": "code",
        "outputId": "a1f2e8f0-e79e-47c4-b7bb-c67c9ed260cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Percents\n",
        "import lexnlp.extract.en.percents\n",
        "\n",
        "isnone = True\n",
        "for x in data.split('\\n'):\n",
        "  a = list(lexnlp.extract.en.percents.get_percents(x))\n",
        "  if len(a) != 0:\n",
        "    isnone = False\n",
        "    print(a)\n",
        "\n",
        "if isnone:\n",
        "    print('None')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUKHHb-RPJ38",
        "colab_type": "code",
        "outputId": "de723dac-0946-40c5-dfa1-da9d7e7a3f02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#PII\n",
        "import lexnlp.extract.en.pii\n",
        "\n",
        "isnone = True\n",
        "for x in data.split('\\n'):\n",
        "  a = list(lexnlp.extract.en.pii.get_pii(x))\n",
        "  if len(a) != 0:\n",
        "    isnone = False\n",
        "    print(a)\n",
        "\n",
        "if isnone:\n",
        "    print('None')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv0yUSxKPgmR",
        "colab_type": "code",
        "outputId": "5e78ab7d-97c8-4d0c-843b-2d4b2b72a44b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Ratios\n",
        "import lexnlp.extract.en.ratios\n",
        "\n",
        "isnone = True\n",
        "for x in data.split('\\n'):\n",
        "  a = list(lexnlp.extract.en.ratios.get_ratios(x))\n",
        "  if len(a) != 0:\n",
        "    isnone = False\n",
        "    print(a)\n",
        "\n",
        "if isnone:\n",
        "    print('None')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsRMr0J9PuPA",
        "colab_type": "code",
        "outputId": "c3538132-3aa7-430f-bd14-864f23eee813",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Regulations\n",
        "import lexnlp.extract.en.regulations\n",
        "\n",
        "isnone = True\n",
        "for x in data.split('\\n'):\n",
        "  a = list(lexnlp.extract.en.regulations.get_regulations(x))\n",
        "  if len(a) != 0:\n",
        "    isnone = False\n",
        "    print(a)\n",
        "\n",
        "if isnone:\n",
        "    print('None')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll-SKLErQYLP",
        "colab_type": "code",
        "outputId": "571cc480-544b-4e2d-936f-7eca0fbf870d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Trademarks\n",
        "import lexnlp.extract.en.trademarks\n",
        "\n",
        "isnone = True\n",
        "for x in data.split('\\n'):\n",
        "  a = list(lexnlp.extract.en.trademarks.get_trademarks(x))\n",
        "  if len(a) != 0:\n",
        "    isnone = False\n",
        "    print(a)\n",
        "\n",
        "if isnone:\n",
        "    print('None')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZL0SCyDcQydH",
        "colab_type": "code",
        "outputId": "d80469a4-8840-4d09-ef6b-bbb5faa77bac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#URL's\n",
        "import lexnlp.extract.en.urls\n",
        "isnone = True\n",
        "for x in data.split('\\n'):\n",
        "  a = list(lexnlp.extract.en.urls.get_urls(x))\n",
        "  if len(a) != 0:\n",
        "    isnone = False\n",
        "    print(a)\n",
        "\n",
        "if isnone:\n",
        "    print('None')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cK1JzH5XUp9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install AllenNLP"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWrolpmfVO-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.predictors.predictor import Predictor\n",
        "entity_predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/fine-grained-ner-model-elmo-2018.12.21.tar.gz\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uR9FOpZIUUqX",
        "colab_type": "code",
        "outputId": "2f7525cf-7372-46f0-f206-9c3b21fccd92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 991
        }
      },
      "source": [
        "\n",
        "def process_allen_results(allen_results):\n",
        "  ents = set()\n",
        "  for word, tag in zip(allen_results[\"words\"], allen_results[\"tags\"]):\n",
        "    if tag != \"O\":\n",
        "      ent_position, ent_type = tag.split(\"-\")\n",
        "      if ent_type == \"PERSON\" or ent_type == \"GPE\":\n",
        "        ents.add((word,ent_type))\n",
        "  return ents\n",
        "\n",
        "for row in data.split(\".\"):\n",
        "  if(len(row) > 3):\n",
        "   ner_results = entity_predictor.predict(sentence=row)\n",
        "   result = process_allen_results(ner_results)\n",
        "   if(len(result) > 0):\n",
        "       print(result)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{('ADAMS', 'PERSON')}\n",
            "{('TANNER', 'PERSON')}\n",
            "{('Sumter', 'GPE')}\n",
            "{('Allen', 'PERSON'), ('Harrison', 'PERSON')}\n",
            "{('Allen', 'PERSON'), ('Harrison', 'PERSON')}\n",
            "{('Harrison', 'PERSON')}\n",
            "{('Allen', 'PERSON'), ('Harrison', 'PERSON'), ('Gainesville', 'GPE')}\n",
            "{('Tennessee', 'GPE'), ('Gainesville', 'GPE')}\n",
            "{('Harrison', 'PERSON')}\n",
            "{('Harrison', 'PERSON')}\n",
            "{('SMITH', 'PERSON')}\n",
            "{('Harrison', 'PERSON')}\n",
            "{('Bos', 'PERSON')}\n",
            "{('Aik', 'PERSON')}\n",
            "{('Harrison', 'PERSON')}\n",
            "{('Chit', 'PERSON')}\n",
            "{('Harrison', 'PERSON')}\n",
            "{('MURPHY', 'PERSON')}\n",
            "{('JONES', 'PERSON'), ('Aik', 'PERSON')}\n",
            "{('Stewart', 'PERSON')}\n",
            "{('Sawyer', 'PERSON')}\n",
            "{('Elliott', 'PERSON'), ('Perkins', 'PERSON')}\n",
            "{('Dane', 'PERSON')}\n",
            "{('Salk', 'PERSON'), ('Poole', 'PERSON')}\n",
            "{('Bos', 'PERSON')}\n",
            "{('Foot', 'PERSON')}\n",
            "{('Aik', 'PERSON')}\n",
            "{('Mansony', 'PERSON')}\n",
            "{('Gary', 'PERSON')}\n",
            "{('ORMOND', 'PERSON')}\n",
            "{('Clay', 'PERSON')}\n",
            "{('Booker', 'PERSON')}\n",
            "{('SAFFOLD', 'PERSON')}\n",
            "{('Ala', 'PERSON'), ('Marshall', 'PERSON')}\n",
            "{('JOHN', 'PERSON'), ('D', 'PERSON')}\n",
            "{('CUNNINGHAM', 'PERSON')}\n",
            "{('Bibb', 'PERSON')}\n",
            "{('JOHN', 'PERSON'), ('D', 'PERSON')}\n",
            "{('CUNNINGHAM', 'PERSON')}\n",
            "{('McKenzie', 'PERSON')}\n",
            "{('Ala', 'PERSON')}\n",
            "{('HALE', 'PERSON')}\n",
            "{('Evans', 'PERSON')}\n",
            "{('21', 'PERSON'), ('Ala', 'PERSON'), ('Lamar', 'PERSON')}\n",
            "{('Ala', 'PERSON')}\n",
            "{('MOORE', 'PERSON')}\n",
            "{('Bowman', 'PERSON')}\n",
            "{('S', 'PERSON'), ('Jacob', 'PERSON')}\n",
            "{('Cohen', 'PERSON')}\n",
            "{('WHITLOCK', 'PERSON')}\n",
            "{('Edwards', 'PERSON')}\n",
            "{('Tenn', 'GPE')}\n",
            "{('county', 'GPE'), ('Weakley', 'GPE')}\n",
            "{('Austin', 'PERSON')}\n",
            "{('Sawyer', 'PERSON')}\n",
            "{('Perkins', 'PERSON')}\n",
            "{('Ala', 'GPE')}\n",
            "{('Stewart', 'PERSON')}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}